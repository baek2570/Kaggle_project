{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mglearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape (15120, 55)\n",
      "test_data shape (565892, 54)\n"
     ]
    }
   ],
   "source": [
    "train_id = train.Id\n",
    "test_id = test.Id\n",
    "# Id 삭제 \n",
    "train.drop('Id', axis = 1, inplace=True)\n",
    "test.drop('Id', axis = 1, inplace=True)\n",
    "\n",
    "print('train_data shape', train.shape)\n",
    "print('test_data shape', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        5\n",
       "1        5\n",
       "2        2\n",
       "3        2\n",
       "4        5\n",
       "5        2\n",
       "6        5\n",
       "7        5\n",
       "8        5\n",
       "9        5\n",
       "10       5\n",
       "11       2\n",
       "12       2\n",
       "13       5\n",
       "14       5\n",
       "15       5\n",
       "16       5\n",
       "17       5\n",
       "18       5\n",
       "19       5\n",
       "20       5\n",
       "21       2\n",
       "22       5\n",
       "23       5\n",
       "24       5\n",
       "25       5\n",
       "26       5\n",
       "27       2\n",
       "28       2\n",
       "29       5\n",
       "        ..\n",
       "15090    3\n",
       "15091    3\n",
       "15092    3\n",
       "15093    6\n",
       "15094    6\n",
       "15095    6\n",
       "15096    6\n",
       "15097    3\n",
       "15098    3\n",
       "15099    6\n",
       "15100    3\n",
       "15101    6\n",
       "15102    3\n",
       "15103    6\n",
       "15104    3\n",
       "15105    6\n",
       "15106    3\n",
       "15107    3\n",
       "15108    3\n",
       "15109    6\n",
       "15110    6\n",
       "15111    6\n",
       "15112    6\n",
       "15113    3\n",
       "15114    3\n",
       "15115    3\n",
       "15116    3\n",
       "15117    3\n",
       "15118    3\n",
       "15119    3\n",
       "Name: Cover_Type, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.Cover_Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 변수 설명\n",
    "- Elevation - Elevation in meters (높이 /meter 단위)\n",
    "- Aspect - Aspect in degrees azimuth (방위각)\n",
    "- Slope - Slope in degrees (기울기 각도 /도 단위)\n",
    "- Horizontal_Distance_To_Hydrology - Horz Dist to nearest surface water features (수원과의 수평거리)\n",
    "- Vertical_Distance_To_Hydrology - Vert Dist to nearest surface water features (수원과의 수직거리)\n",
    "- Horizontal_Distance_To_Roadways - Horz Dist to nearest roadway (길가와의 수평거리)\n",
    "- Hillshade_9am (0 to 255 index) - Hillshade index at 9am, summer solstice (오전 9시의 차양 / 0~255)\n",
    "- Hillshade_Noon (0 to 255 index) - Hillshade index at noon, summer solstice (정오시의 차양/ 0~255)\n",
    "- Hillshade_3pm (0 to 255 index) - Hillshade index at 3pm, summer solstice (오후 9시의 차양/ 0~255)\n",
    "- Horizontal_Distance_To_Fire_Points - Horz Dist to nearest wildfire ignition points (야생 산불 발화지점과의 수평거리)\n",
    "- Wilderness_Area (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation (황야 지대 /4종류 ) in Roosevelt National Forest of northern Colorado\n",
    "- Soil_Type (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation (토양 종류 / 40종류)\n",
    "\n",
    "- 토양종류와 황야 지대 카테고리별 설명은 https://www.kaggle.com/c/forest-cover-type-prediction/data 참조\n",
    "\n",
    "\n",
    "## 종속 변수\n",
    "- Cover_Type (7 types, integers 1 to 7) - Forest Cover Type designation- (산림 유형 / 7종류) (the predominant kind of tree cover)\n",
    "- 1 - Spruce/Fir\n",
    "- 2 - Lodgepole Pine\n",
    "- 3 - Ponderosa Pine\n",
    "- 4 - Cottonwood/Willow\n",
    "- 5 - Aspen\n",
    "- 6 - Douglas-fir\n",
    "- 7 - Krummholz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_data shape (581012, 54)\n"
     ]
    }
   ],
   "source": [
    "# 독립변수와 종속변수 나누기\n",
    "train_data = train.iloc[:, :-1]\n",
    "train_target = train.iloc[:, -1]\n",
    "# train 데이터 갯수 저장\n",
    "ntrain = len(train)\n",
    "\n",
    "# train, test데이터 합쳐서 같이 feature engineering\n",
    "all_data = pd.concat([train_data, test])\n",
    "print('all_data shape', all_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# type 변수들 관찰\n",
    "- Soil_type7, 8, 15, 25는 훈련데이터에서 한 카테고리가 1 이하로 나왔으므로 의미가 없다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wilderness_Area1     3597\n",
       "Wilderness_Area2      499\n",
       "Wilderness_Area3     6349\n",
       "Wilderness_Area4     4675\n",
       "Soil_Type1            355\n",
       "Soil_Type2            623\n",
       "Soil_Type3            962\n",
       "Soil_Type4            843\n",
       "Soil_Type5            165\n",
       "Soil_Type6            650\n",
       "Soil_Type7              0\n",
       "Soil_Type8              1\n",
       "Soil_Type9             10\n",
       "Soil_Type10          2142\n",
       "Soil_Type11           406\n",
       "Soil_Type12           227\n",
       "Soil_Type13           476\n",
       "Soil_Type14           169\n",
       "Soil_Type15             0\n",
       "Soil_Type16           114\n",
       "Soil_Type17           612\n",
       "Soil_Type18            60\n",
       "Soil_Type19            46\n",
       "Soil_Type20           139\n",
       "Soil_Type21            16\n",
       "Soil_Type22           345\n",
       "Soil_Type23           757\n",
       "Soil_Type24           257\n",
       "Soil_Type25             1\n",
       "Soil_Type26            54\n",
       "Soil_Type27            15\n",
       "Soil_Type28             9\n",
       "Soil_Type29          1291\n",
       "Soil_Type30           725\n",
       "Soil_Type31           332\n",
       "Soil_Type32           690\n",
       "Soil_Type33           616\n",
       "Soil_Type34            22\n",
       "Soil_Type35           102\n",
       "Soil_Type36            10\n",
       "Soil_Type37            34\n",
       "Soil_Type38           728\n",
       "Soil_Type39           657\n",
       "Soil_Type40           459\n",
       "Cover_Type          60480\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[:, 'Wilderness_Area1':'Cover_Type'].sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_columns = ['Soil_Type7', 'Soil_Type8', 'Soil_Type15', 'Soil_Type25']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.loc[:'Wilderness_Area1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data.drop(drop_columns, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- feature engineering이 끝났으면 다시 test, train 데이터로 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15120, 55)\n",
      "(565892, 50)\n"
     ]
    }
   ],
   "source": [
    "train_data = all_data[:ntrain]\n",
    "test = all_data[ntrain:]\n",
    "print(train.shape)\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score, ABCMeta\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import *\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15120,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "encoder = OneHotEncoder()\n",
    "train_target_onehot = encoder.fit_transform(train_target.reshape(-1, 1)).toarray()\n",
    "train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_score(model):\n",
    "    cv = KFold(n_splits=5, shuffle = True, random_state = 42).get_n_splits(train_data.values)\n",
    "    return cross_val_score(model, train_data, train_target, scoring='accuracy', cv= cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.71494709  0.72685185  0.70634921  0.72519841  0.8098545 ]\n",
      "SVC : 0.7366(0.0374)\n"
     ]
    }
   ],
   "source": [
    "svc = make_pipeline(StandardScaler(), SVC(C=1000, gamma = 0.1))\n",
    "\n",
    "score = cv_score(svc)\n",
    "print(score)\n",
    "print('SVC : {:.4f}({:.4f})'.format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.74768519  0.71990741  0.75892857  0.77612434  0.83796296]\n",
      "GBoost : 0.7681(0.0394)\n",
      "[ 0.71957672  0.71593915  0.74272487  0.7853836   0.8260582 ]\n",
      "RandomForest : 0.7579(0.0421)\n"
     ]
    }
   ],
   "source": [
    "GBoost = GradientBoostingClassifier(n_estimators=3000, max_depth = 4, learning_rate =0.01, min_samples_leaf=15,\n",
    "                                   min_samples_split = 10, random_state = 5)\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=300, max_depth=15, min_samples_leaf=2, random_state=5, n_jobs=-1)\n",
    "score = cv_score(GBoost)\n",
    "print(score)\n",
    "print('GBoost : {:.4f}({:.4f})'.format(score.mean(), score.std()))\n",
    "\n",
    "score = cv_score(RF)\n",
    "print(score)\n",
    "print('RandomForest : {:.4f}({:.4f})'.format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.820105820106\n",
      "0.851587301587\n",
      "0.842592592593\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_target, random_state = 42)\n",
    "\n",
    "print(svc.fit(X_train,y_train).score(X_test, y_test))\n",
    "print(GBoost.fit(X_train,y_train).score(X_test, y_test))\n",
    "print(RF.fit(X_train,y_train).score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1f39ae34c8c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_target_onehot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scaler' is not defined"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_target_onehot, random_state = 42)\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "nb_classes = 7\n",
    "X = tf.placeholder(tf.float32, [None, X_train.shape[1]])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.get_variable(\"WWi111\", shape =[X_train.shape[1], 200], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([200]), name = 'bias1')\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"WWi222\", shape =[200, 300], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([300]), name = 'bias2')\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"WWi333\", shape =[300, 200], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([200]), name = 'bias3')\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"WWi444\", shape =[200, nb_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([nb_classes]), name = 'bias4')\n",
    "hypothesis = tf.matmul(L3, W4) + b4\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis, labels = Y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(cost)\n",
    "prediction = tf.armax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(2000):\n",
    "        \n",
    "        c , _ = sess.run([cost, optimizer], feed_dict = {X: X_train, Y: y_train, keep_prob: 0.7})\n",
    "        if step  % 100 == 0:\n",
    "            print('step : {}, cost = {}'.format(step, c))\n",
    "    print('train set : ', sess.run(accuracy, feed_dict = {X: X_train, Y: y_train, keep_prob: 1.0}))\n",
    "    acc = sess.run(accuracy, feed_dict = {X: X_test, Y: y_test, keep_prob: 1.0})    \n",
    "    print('test set : ', acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 그리드 서치\n",
    "- for xgboost\n",
    "- 다른 모델도 추후 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gsearch2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-aacb12674cd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgsearch2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgsearch2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gsearch2' is not defined"
     ]
    }
   ],
   "source": [
    "param_test = {\n",
    " 'max_depth':[4,5,6, 7],\n",
    " 'min_child_weight':range(4, 10, 2)\n",
    "}\n",
    "cv = KFold(5, shuffle = True, random_state=42)\n",
    "model_xgb = xgb.XGBClassifier(n_estimators=200, objective='multi:softmax', random_state =7, nthread = -1)\n",
    "grid = GridSearchCV(model_xgb, param_test, scoring='accuracy', n_jobs=-1, cv = cv)\n",
    "\n",
    "grid.fit(train_data, train_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.81012, std: 0.00651, params: {'max_depth': 4, 'min_child_weight': 4},\n",
       "  mean: 0.81290, std: 0.00758, params: {'max_depth': 4, 'min_child_weight': 6},\n",
       "  mean: 0.81138, std: 0.00772, params: {'max_depth': 4, 'min_child_weight': 8},\n",
       "  mean: 0.83280, std: 0.00710, params: {'max_depth': 5, 'min_child_weight': 4},\n",
       "  mean: 0.83036, std: 0.00866, params: {'max_depth': 5, 'min_child_weight': 6},\n",
       "  mean: 0.82996, std: 0.00806, params: {'max_depth': 5, 'min_child_weight': 8},\n",
       "  mean: 0.84405, std: 0.00911, params: {'max_depth': 6, 'min_child_weight': 4},\n",
       "  mean: 0.84008, std: 0.00701, params: {'max_depth': 6, 'min_child_weight': 6},\n",
       "  mean: 0.83862, std: 0.00864, params: {'max_depth': 6, 'min_child_weight': 8},\n",
       "  mean: 0.85126, std: 0.00684, params: {'max_depth': 7, 'min_child_weight': 4},\n",
       "  mean: 0.85112, std: 0.00687, params: {'max_depth': 7, 'min_child_weight': 6},\n",
       "  mean: 0.84663, std: 0.00982, params: {'max_depth': 7, 'min_child_weight': 8}],\n",
       " {'max_depth': 7, 'min_child_weight': 4},\n",
       " 0.85125661375661377)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.grid_scores_, grid.best_params_, grid.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
